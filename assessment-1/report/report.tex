\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[table]{xcolor}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage[flushleft]{threeparttable}
\graphicspath{{./figures/}}
\usepackage{array}
\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\begin{document}

\title{Reinforcement Learning with Human Feedback Coursework Report}

\author{\IEEEauthorblockN{Liyou Zhou}
    \IEEEauthorblockA{\textit{Department of Computer Science} \\
        \textit{University of Lincoln}\\
        Lincoln, UK \\
        https://orcid.org/0009-0005-9491-9003}
}

\maketitle

\begin{abstract}
Human feedback has been shown to improve the learning outcomes of Reinforcement Learning algorithms. In this report, the effect of human feedback on a simple grid world navigation problem is explored. A basic Q-learning algorithm is implemented and augmented with implicit or explicit human feedback. The results show that human feedback reduces the number of steps required for the agent to learn an optimal policy. The explicit feedback strategy showed an advantage in the early stages of learning over the implicit case but both converged in similar number of steps. The human feedback strategies are much slower in wall clock time compared to the strategy without feedback. Explicit feedback is richer in information but is more cognitively taxing for the human teacher. It is slower per feedback request but requires fewer requests overall.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, Human Feedback
\end{IEEEkeywords}

\section{Introduction}

When interacting with the real world, robots usually follow a observation-decision-action cycle. They observe the environment, make decisions based on the observations, and then act on the environment. The state of the robot and the environment depends on all the previous actions of the robot, and any action taken will affect the probability distribution of future states. This class of problem is known as Sequential Decision Problems (SDPs). Markov Decision Processes (MDPs) \cite{putermanMarkovDecisionProcesses2014} are often used to describe such problems. MPDs assumes fully accurate observations of the environment but actions have non-deterministic outcomes.

Reinforcement Learning (RL) is a subfield of machine learning that deals with sequential decision problems. An learning agent is usually made to interact with an environment in order to an optimal control policy. The agent receives rewards from the environment as feedback for its actions. The goal of the agent is to maximize the cumulative reward over time.

Robot navigation problems is a typical example of SDPs and are often modelled using MDPs. It has been demonstrated that RL algorithms solves navigation problems effectively \cite{zhuDeepReinforcementLearning2021}. There is usually a reward associated with reaching the navigation goal. An RL algorithm can autonomously explore the state/action space of the environment and learn an optimal policy to reach the goal.

A main weakness of such approaches is that the reward is sparse. Only the goal state has a non-zero reward. At the beginning of learning, the agent spend a lot of time wondering aimlessly in the environment without receiving any reward. A human processes domain knowledge about the environment such as where the goal is and where the obstacles to avoid. By interacting with a human teacher in the learning process, the human can use some of the domain knowledge to guide the learning of the agent to speed up the learning process.

In this report, we use a simple grid world with a single goal state to explore the effect of human guidance in the Reinforcement Learning process. We compare the performance of the agent with and without human guidance as well as different granularity of feedback.

\section{Related Work}

Corrective feedback is understood to be an important part of learning. \cite{ellisIMPLICITEXPLICITCORRECTIVE2006} examined the effect of corrective feedback on language learning. It tested 2 types of feedback, implicit and explicit. Implicit feedback provide only a good/bad response to error. Explicit feedback, on the other hand, provide detailed explanation relating to the error itself. The study found that explicit feedback showed a clear advantage over implicit feedback.

Human feedback has been widely used in Reinforcement Learning \cite{liHumanCenteredReinforcementLearning2019} to improve learning outcomes. \cite{iidaGeneratingPersonalityCharacter1998} showed that by tweaking reward according to human feedback, a Q learning algorithm is able to generate personalized control policy for a robotic face. \cite{tenorio-gonzalezDynamicRewardShaping2010} showed that even inconstant feedback improves learning outcomes.

Human feedback is equally applicable in deep reinforcement learning tasks where the state and observation space is large and complex. \cite{j.wuHumanGuidedReinforcementLearning2023} shows that human interventions in an autonomous vehicle can serve as a reward shaping mechanism to improve the performance of the agent. \cite{zhaDistillingRetrievingGeneralizable2024} demonstrated the use of natural language feedback for robotic manipulation tasks. It showed improvements in high level task planning as well as low level control policy.

\section{Simulation Setup}

The simulation environment is called Frozen Lake and comes from the software package Gymnasium\cite{towersGymnasium2024}. It emulates a robot navigation problem in a grid world. An screen shot from the environment is shown in Figure \ref{fig:frozen_lake_overview}. The environment is a 4x4 grid world with consists of navigable tiles, hole tiles and a goal tile. The agent start from tile [0, 0] and tries to reach the goal at tile [3, 3] without falling into a hole.

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{frozen_lake_overview.png}
    \caption{Frozen Lake Grid World}
    \label{fig:frozen_lake_overview}
\end{figure}

\subsubsection{Action Space}

At each iteration, the agent can take one of four actions: move up, move down, move left, and move right. If the agent tries to move off the grid, it stays in the same location. For the purpose of simplification and emphasize the effect of feedback, the agent moves deterministically.

\subsubsection{Observation Space}

The agent can observe its current location in the grid world as a single number. It is calculated as \(current\_row \times nrows + current\_col\).

\subsubsection{Rewards}

The agent receives a reward of 1 when it reaches the goal and a reward of 0 otherwise. Either reaching a tile with a hole or the goal tile terminates the episode.

\section{Methodology}

\subsubsection{Basic Q-learning}

A basic Q-learning\cite{watkinsQlearning1992} algorithm is implemented to solve the Frozen Lake problem. The temporal difference update equation is as follows:

\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \label{eq:q_learning}
\end{equation}
Where:
\begin{conditions}
    Q(s, a) & The Q-value of state \(s\) and action \(a\) \\
    \alpha  & The learning rate \\
    r       & The reward received after taking action \(a\) in state \(s\) \\
    \gamma  & The discount factor \\
    s'      & The next state \\
    a'      & The next action
\end{conditions}

\subsubsection{Implicit Feedback}

In the implicit feedback setting, at each iteration, the agent solicit human feedback after taking an action with the question: \textit{"Was the agent's action correct? (y/n)"}. If the human answers "y", 1 is added to the reward. Nothing is done otherwise. The agent then updates the Q-value using the environment reward as well as the human feedback rewards. Eq.~\ref{eq:q_learning} then becomes:

\begin{multline}
    Q(s, a) \leftarrow Q(s, a) + \\
    \alpha \left( r + F(s, s') + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \label{eq:implicit_feedback}
\end{multline}
Where:
\begin{conditions}
    F(s, s') & Human feedback reward \\
    s        & The current state \\
    s'       & The next state \\
\end{conditions}

The human feedback is cached with \(s\) and \(s'\) as keys. This is done to minimize the number of times the human input is required during training.

\subsubsection{Explicit Feedback}

In the explicit feedback setting, the agent solicit human feedback after taking an action with the question: \textit{"What should the agent have done instead?"}. In this instance the human provides the optimal action \(a^*\) for state \(s\). For every iterations, the agent is then able to perform 2 Q value updates. Once following Eq.~\ref{eq:implicit_feedback} with a human feedback reward of 1 if the action taken \(a\) is the same as the optimal action \(a^*\). The second update is simpler:

\begin{equation}
    Q(s, a^*) \leftarrow (1-\alpha) \times Q(s, a^*) + \alpha \times r
    \label{eq:explicit_feedback}
\end{equation}

This serves to push up the Q value of the optimal action \(a^*\) in state \(s\) early on the in the learning in promotion of convergence.

\section{Results}

Experiments were run with Q learning algorithm on the Frozen Lake environment. The learning rate \(\alpha\) was set to 0.01 and the discount factor \(\gamma\) was set to 0.95. The number of episodes was set to 2000. 3 variation of the algorithm is tested: Q-learning without feedback, Q-learning with implicit human feedback, and Q-learning with explicit human feedback. Once the agents have converged, the optimal policy is extracted and shown in Figure \ref{fig:optimal_policy}. The learning curve is shown in Figure \ref{fig:learning_curve_steps}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{optimal_policy.png}
    \caption{Converged optimal policy of Q-learning on the Frozen Lake environment without feedback compared to Implicit and Explicit Feedback.}
    \label{fig:optimal_policy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{learning_curve_steps.png}
    \caption{Learning Curve of Q-learning on the Frozen Lake environment without feedback compared to Implicit and Explicit Feedback. Mean reward is calculated as a rolling average of 100 episodes.}
    \label{fig:learning_curve_steps}
\end{figure}

\section{Discussions}

The 3 learning strategies converged to slightly different final policies. Since the execution of the agent is deterministic, the policies are equally optimal. In any of the policies, starting from any of the tiles, the agent would have arrived at the goal. Although sometime taking more steps than necessary. This is because no extra reward was given for shorter routes during training. The only objective as to arrive at the goal without falling into a hole. The human feedback could have played a hand in nudging the agent towards a particular policy.

The learning curves shows a clear advantage for the addition of human feedback, especially in the early stages of learning. The agent without feedback took a long time wondering around the environment without receiving any reward. This means there is no learning signal to improve the policy. Hence this initial time is wasted. In the case of human feedback, the agent is given extra reward to guide it towards the goal. The agent was able to quickly pick up goal reward and start improving the policy. Both the explicit and implicit feedback experiments converged in fewer steps than the agent without feedback.

Comparing the learning curve for explicit and implicit feedback, the explicit feedback showed a clear advantage in the early stages of learning. This fits the expectation as explicit feedback contains more information. The explicit feedback algorithm leverages this extra information and performs an additional Q value update on top of what the implicit feedback algorithm does.
However this early advantage tapered off and the 2 strategies converged in similar number of steps. This could be that the extra reward update is not scaled appropriately and the algorithm suffered reward shaping problem. The update in Eq.~\ref{eq:explicit_feedback} could have pushed the Q value too high above equilibrium, needing extra steps to scale it down.

Even though the human feedback sped up learning in compute time (number of steps to converge), they are actually much slower in wall clock time. The request for human feedback is several orders of magnitude slower than the agent's autonomous exploration. In the experiments, the strategy without feedback would learn in under half a minute while human feedback strategies will take around 3 minutes. Human feedback is only worthwhile in particular cases where the agent is unable to explore the environment effectively or where interaction with the environment is expensive.

The number of human feedback request is minimized through caching of response. In the implicit case, human feedback is given for every state/action pair which means there is at most \(16 \times 4 = 64\) requests. In the explicit case, human feedback is given for every state which means there is at most 16 requests. However, it was noted that explicit feedback is more cognitively taxing, and takes longer to respond to for the human teacher.

\cite{wiewioraPotentialBasedShapingQValue2003} demonstrated that consistent reward feedback signal during learning is equivalent to initializing the Q values with human provided values in terms of the learning outcome. However, initializing Q values before training is much quicker and less error prone than interactively providing feedback. This is especially true in the case of explicit feedback where the human teacher has to provide the optimal action for every state.

\section{Conclusion}

Human feedback reduces the number of steps required for an agent to learn an optimal policy in a grid world navigation problem. The feedback can be implicit or explicit. The explicit feedback strategy showed an advantage in the early stages of learning but converged in similar number of steps as the implicit feedback strategy. The human feedback strategies are much slower in wall clock time compared to the strategy without feedback. Explicit feedback is richer in information but is more cognitively taxing for the human teacher. It is slower per feedback request but requires fewer requests overall.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,AR.bib}

\end{document}

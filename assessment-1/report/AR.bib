@article{amershiPowerPeopleRole2014,
  title = {Power to the {{People}}: {{The Role}} of {{Humans}} in {{Interactive Machine Learning}}},
  shorttitle = {Power to the {{People}}},
  author = {Amershi, Saleema and Cakmak, Maya and Knox, W. and Kulesza, Todd},
  year = {2014},
  month = dec,
  journal = {Ai Magazine},
  volume = {35},
  pages = {105--120},
  doi = {10.1609/aimag.v35i4.2513},
  abstract = {Systems that can learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that demonstrate how interactivity results in a tight coupling between the system and the user, exemplify ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. After giving a glimpse of the progress that has been made thus far, we discuss some of the challenges we face in moving the field forward.},
  file = {/home/liyouzhou/Zotero/storage/W72TK6FU/Amershi et al. - 2014 - Power to the People The Role of Humans in Interac.pdf}
}

@inproceedings{bradleyknoxTAMERTrainingAgent2008,
  title = {{{TAMER}}: {{Training}} an {{Agent Manually}} via {{Evaluative Reinforcement}}},
  shorttitle = {{{TAMER}}},
  booktitle = {2008 7th {{IEEE International Conference}} on {{Development}} and {{Learning}}},
  author = {Bradley Knox, W. and Stone, Peter},
  year = {2008},
  month = aug,
  pages = {292--297},
  issn = {2161-9476},
  doi = {10.1109/DEVLRN.2008.4640845},
  urldate = {2024-05-01},
  abstract = {Though computers have surpassed humans at many tasks, especially computationally intensive ones, there are many tasks for which human expertise remains necessary and/or useful. For such tasks, it is desirable for a human to be able to transmit knowledge to a learning agent as quickly and effortlessly as possible, and, ideally, without any knowledge of the details of the agent's learning process. This paper proposes a general framework called Training an Agent Manually via Evaluative Reinforcement (TAMER) that allows a human to train a learning agent to perform a common class of complex tasks simply by giving scalar reward signals in response to the agent's observed actions. Specifically, in sequential decision making tasks, an agent models the human's reward function and chooses actions that it predicts will receive the most reward. Our novel algorithm is fully implemented and tested on the game Tetris. Leveraging the human trainers' feedback, the agent learns to clear an average of more than 50 lines by its third game, an order of magnitude faster than the best autonomous learning agents.},
  keywords = {Approximation algorithms,Games,Humans,Learning,Robots,Supervised learning,Training},
  file = {/home/liyouzhou/Zotero/storage/L2AB282Z/Bradley Knox and Stone - 2008 - TAMER Training an Agent Manually via Evaluative R.pdf;/home/liyouzhou/Zotero/storage/7YH588TT/4640845.html}
}

@misc{chiangLearningNavigationBehaviors2019,
  title = {Learning {{Navigation Behaviors End-to-End}} with {{AutoRL}}},
  author = {Chiang, Hao-Tien Lewis and Faust, Aleksandra and Fiser, Marek and Francis, Anthony},
  year = {2019},
  month = feb,
  number = {arXiv:1809.10124},
  eprint = {1809.10124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1809.10124},
  urldate = {2024-05-01},
  abstract = {We learn end-to-end point-to-point and path-following navigation behaviors that avoid moving obstacles. These policies receive noisy lidar observations and output robot linear and angular velocities. The policies are trained in small, static environments with AutoRL, an evolutionary automation layer around Reinforcement Learning (RL) that searches for a deep RL reward and neural network architecture with large-scale hyper-parameter optimization. AutoRL first finds a reward that maximizes task completion, and then finds a neural network architecture that maximizes the cumulative of the found reward. Empirical evaluations, both in simulation and on-robot, show that AutoRL policies do not suffer from the catastrophic forgetfulness that plagues many other deep reinforcement learning algorithms, generalize to new environments and moving obstacles, are robust to sensor, actuator, and localization noise, and can serve as robust building blocks for larger navigation tasks. Our path-following and point-to-point policies are respectively 23\% and 26\% more successful than comparison methods across new environments. Video at: https://youtu.be/0UwkjpUEcbI},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/YJ9U2XY2/Chiang et al. - 2019 - Learning Navigation Behaviors End-to-End with Auto.pdf;/home/liyouzhou/Zotero/storage/4Z4IJF3L/1809.html}
}

@article{ellisIMPLICITEXPLICITCORRECTIVE2006,
  title = {{{IMPLICIT AND EXPLICIT CORRECTIVE FEEDBACK AND THE ACQUISITION OF L2 GRAMMAR}}},
  author = {Ellis, Rod and Loewen, Shawn and Erlam, Rosemary},
  year = {2006},
  month = jun,
  journal = {Studies in Second Language Acquisition},
  volume = {28},
  number = {2},
  pages = {339--368},
  issn = {1470-1545, 0272-2631},
  doi = {10.1017/S0272263106060141},
  urldate = {2024-05-02},
  abstract = {This article reviews previous studies of the effects of implicit and  explicit corrective feedback on SLA, pointing out a number of  methodological problems. It then reports on a new study of the effects of  these two types of corrective feedback on the acquisition of past tense  -ed. In an experimental design (two experimental groups and a  control group), low-intermediate learners of second language English  completed two communicative tasks during which they received either  recasts (implicit feedback) or metalinguistic explanation (explicit  feedback) in response to any utterance that contained an error in the  target structure. Acquisition was measured by means of an oral imitation  test (designed to measure implicit knowledge) and both an untimed  grammaticality judgment test and a metalinguistic knowledge test (both  designed to measure explicit knowledge). The tests were administered prior  to the instruction, 1 day after the instruction, and again 2 weeks later.  Statistical comparisons of the learners' performance on the posttests  showed a clear advantage for explicit feedback over implicit feedback for  both the delayed imitation and grammaticality judgment posttests. Thus,  the results indicate that metalinguistic explanation benefited implicit as  well as explicit knowledge and point to the importance of including  measures of both types of knowledge in experimental studies.This research was funded by a Marsden Fund grant  awarded by the Royal Society of Arts of New Zealand. Researchers other  than the authors who contributed to the research were Jenefer Philip,  Satomi Mizutami, Keiko Sakui, and Thomas Delaney. Thanks go to the editors  of this special issue and to two anonymous SSLA reviewers of a  draft of the article for their constructive comments.},
  langid = {english},
  file = {/home/liyouzhou/Zotero/storage/TQKWG3KE/Ellis et al. - 2006 - IMPLICIT AND EXPLICIT CORRECTIVE FEEDBACK AND THE .pdf}
}

@article{esserGuidedReinforcementLearning2023a,
  title = {Guided {{Reinforcement Learning}}: {{A Review}} and {{Evaluation}} for {{Efficient}} and {{Effective Real-World Robotics}} [{{Survey}}]},
  shorttitle = {Guided {{Reinforcement Learning}}},
  author = {E{\ss}er, Julian and Bach, Nicolas and Jestel, Christian and Urbann, Oliver and Kerner, S{\"o}ren},
  year = {2023},
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {30},
  number = {2},
  pages = {67--85},
  issn = {1558-223X},
  doi = {10.1109/MRA.2022.3207664},
  urldate = {2024-02-21},
  abstract = {Recent successes aside, reinforcement learning (RL) still faces significant challenges in its application to the real-world robotics domain. Guiding the learning process with additional knowledge offers a potential solution, thus leveraging the strengths of data- and knowledge-driven approaches. However, this field of research encompasses several disciplines and hence would benefit from a structured overview. In this article, we propose a concept of guided RL that provides a systematic approach toward accelerating the training process and improving performance for real-world robotics settings. We introduce a taxonomy that structures guided RL approaches and shows how different sources of knowledge can be integrated into the learning pipeline in a practical way. Based on this, we describe available approaches in this field and quantitatively evaluate their specific impact in terms of efficiency, effectiveness, and sim-to-real transfer within the robotics domain.},
  keywords = {Automation,Computational modeling,Reinforcement learning,Robots,Task analysis},
  file = {/home/liyouzhou/Zotero/storage/PSTASPRD/EÃŸer et al. - 2023 - Guided Reinforcement Learning A Review and Evalua.pdf;/home/liyouzhou/Zotero/storage/REQVKZNI/9926159.html}
}

@inproceedings{iidaGeneratingPersonalityCharacter1998,
  title = {Generating Personality Character in a Face Robot through Interaction with Human},
  booktitle = {7th {{IEEE International Workshop}} on {{Robot}} and {{Human Communication}}},
  author = {Iida, F and Tabata, M and Hara, F},
  year = {1998},
  pages = {481--486}
}

@article{j.wuHumanGuidedReinforcementLearning2023,
  title = {Human-{{Guided Reinforcement Learning With Sim-to-Real Transfer}} for {{Autonomous Navigation}}},
  author = {{J. Wu} and {Y. Zhou} and {H. Yang} and {Z. Huang} and {C. Lv}},
  year = {2023},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {12},
  pages = {14745--14759},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3314762}
}

@misc{liangLearningLearnFaster2024,
  title = {Learning to {{Learn Faster}} from {{Human Feedback}} with {{Language Model Predictive Control}}},
  author = {Liang, Jacky and Xia, Fei and Yu, Wenhao and Zeng, Andy and Arenas, Montserrat Gonzalez and Attarian, Maria and Bauza, Maria and Bennice, Matthew and Bewley, Alex and Dostmohamed, Adil and Fu, Chuyuan Kelly and Gileadi, Nimrod and Giustina, Marissa and Gopalakrishnan, Keerthana and Hasenclever, Leonard and Humplik, Jan and Hsu, Jasmine and Joshi, Nikhil and Jyenis, Ben and Kew, Chase and Kirmani, Sean and Lee, Tsang-Wei Edward and Lee, Kuang-Huei and Michaely, Assaf Hurwitz and Moore, Joss and Oslund, Ken and Rao, Dushyant and Ren, Allen and Tabanpour, Baruch and Vuong, Quan and Wahid, Ayzaan and Xiao, Ted and Xu, Ying and Zhuang, Vincent and Xu, Peng and Frey, Erik and Caluwaerts, Ken and Zhang, Tingnan and Ichter, Brian and Tompson, Jonathan and Takayama, Leila and Vanhoucke, Vincent and Shafran, Izhak and Mataric, Maja and Sadigh, Dorsa and Heess, Nicolas and Rao, Kanishka and Stewart, Nik and Tan, Jie and Parada, Carolina},
  year = {2024},
  month = feb,
  number = {arXiv:2402.11450},
  eprint = {2402.11450},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.11450},
  urldate = {2024-02-21},
  abstract = {Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9\% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5\%. See videos, code, and demos at: https://robot-teaching.github.io/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/J7X5K525/Liang et al. - 2024 - Learning to Learn Faster from Human Feedback with .pdf;/home/liyouzhou/Zotero/storage/FXRM4J2F/2402.html}
}

@article{liHumanCenteredReinforcementLearning2019,
  title = {Human-{{Centered Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Human-{{Centered Reinforcement Learning}}},
  author = {Li, Guangliang and Gomez, Randy and Nakamura, Keisuke and He, Bo},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Human-Machine Systems},
  volume = {49},
  number = {4},
  pages = {337--349},
  issn = {2168-2305},
  doi = {10.1109/THMS.2019.2912447},
  urldate = {2024-02-21},
  abstract = {Human-centered reinforcement learning (RL), in which an agent learns how to perform a task from evaluative feedback delivered by a human observer, has become more and more popular in recent years. The advantage of being able to learn from human feedback for a RL agent has led to increasing applicability to real-life problems. This paper describes the state-of-the-art human centered RL algorithms and aims to become a starting point for researchers who are initiating their endeavors in human-centered RL. Moreover, the objective of this paper is to present a comprehensive survey of the recent breakthroughs in this field and provide references to the most interesting and successful works. After starting with an introduction of the concepts of RL from environmental reward, this paper discusses the origins of human-centered RL and its difference from traditional RL. Then we describe different interpretations of human evaluative feedback, which have produced many human-centered RL algorithms in the past decade. In addition, we describe research on agents learning from both human evaluative feedback and environmental rewards as well as on improving the efficiency of human-centered RL. Finally, we conclude with an overview of application areas and a discussion of future work and open questions.},
  keywords = {Human agent/robot interaction,human reward,interactive reinforcement learning (RL),interactive shaping,Man-machine systems,policy shaping,Programming,Reinforcement learning,Robot learning,Search methods,Standards,Task analysis},
  file = {/home/liyouzhou/Zotero/storage/T54BJQ77/Li et al. - 2019 - Human-Centered Reinforcement Learning A Survey.pdf}
}

@article{linReviewInteractiveReinforcement2020,
  title = {A {{Review}} on {{Interactive Reinforcement Learning From Human Social Feedback}}},
  author = {Lin, Jinying and Ma, Zhen and Gomez, Randy and Nakamura, Keisuke and He, Bo and Li, Guangliang},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {120757--120765},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3006254},
  urldate = {2024-02-21},
  abstract = {Reinforcement learning agent learns how to perform a task by interacting with the environment. The use of reinforcement learning in real-life applications has been limited because of the sample efficiency problem. Interactive reinforcement learning has been developed to speed up the agent's learning and facilitate to learn from ordinary people by allowing them to provide social feedback, e.g, evaluative feedback, advice or instruction. Inspired by real-life biological learning scenarios, there could be many ways to provide feedback for agent learning, such as via hardware delivered, natural interaction like facial expressions, speech or gestures. The agent can even learn from feedback via unimodal or multimodal sensory input. This paper reviews methods for interactive reinforcement learning agent to learn from human social feedback and the ways of delivering feedback. Finally, we discuss some open problems and possible future research directions.},
  keywords = {Feeds,Human agent/robot interaction,interactive reinforcement learning,interactive shaping,Numerical models,Reinforcement learning,Robots,social interaction,Standards,Task analysis,Training},
  file = {/home/liyouzhou/Zotero/storage/GGK9MJJE/Lin et al. - 2020 - A Review on Interactive Reinforcement Learning Fro.pdf}
}

@book{putermanMarkovDecisionProcesses2014,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author = {Puterman, Martin L},
  year = {2014},
  publisher = {John Wiley \& Sons},
  isbn = {1-118-62587-0}
}

@inproceedings{tenorio-gonzalezDynamicRewardShaping2010,
  title = {Dynamic {{Reward Shaping}}: {{Training}} a {{Robot}} by {{Voice}}},
  shorttitle = {Dynamic {{Reward Shaping}}},
  booktitle = {Advances in {{Artificial Intelligence}} -- {{IBERAMIA}} 2010},
  author = {{Tenorio-Gonzalez}, Ana C. and Morales, Eduardo F. and {Villase{\~n}or-Pineda}, Luis},
  editor = {{Kuri-Morales}, Angel and Simari, Guillermo R.},
  year = {2010},
  pages = {483--492},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-16952-6_49},
  abstract = {Reinforcement Learning is commonly used for learning tasks in robotics, however, traditional algorithms can take very long training times. Reward shaping has been recently used to provide domain knowledge with extra rewards to converge faster. The reward shaping functions are normally defined in advance by the user and are static. This paper introduces a dynamic reward shaping approach, in which these extra rewards are not consistently given, can vary with time and may sometimes be contrary to what is needed for achieving a goal. In the experiments, a user provides verbal feedback while a robot is performing a task which is translated into additional rewards. It is shown that we can still guarantee convergence as long as most of the shaping rewards given per state are consistent with the goals and that even with fairly noisy interaction the system can still produce faster convergence times than traditional reinforcement learning techniques.},
  isbn = {978-3-642-16952-6},
  langid = {english}
}

@inproceedings{thomazLearningObjectsHuman2009,
  title = {Learning about Objects with Human Teachers},
  booktitle = {2009 4th {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}} ({{HRI}})},
  author = {Thomaz, Andrea L. and Cakmak, Maya},
  year = {2009},
  month = mar,
  pages = {15--22},
  issn = {2167-2148},
  doi = {10.1145/1514095.1514101},
  urldate = {2024-05-01},
  abstract = {A general learning task for a robot in a new environment is to learn about objects and what actions/effects they afford. To approach this, we look at ways that a human partner can intuitively help the robot learn, Socially Guided Machine Learning. We present experiments conducted with our robot, Junior, and make six observations characterizing how people approached teaching about objects. We show that Junior successfully used transparency to mitigate errors. Finally, we present the impact of ``social'' versus ``non-social'' data sets when training SVM classifiers.},
  keywords = {Complexity theory,Education,Grasping,Humans,Interactive Machine Learning,Machine learning,Robots,Social Robot Learning,Systematics},
  file = {/home/liyouzhou/Zotero/storage/FQKNQH4S/Thomaz and Cakmak - 2009 - Learning about objects with human teachers.pdf;/home/liyouzhou/Zotero/storage/IQ5WAW6A/6256013.html}
}

@misc{towersGymnasium2024,
  title = {Gymnasium},
  author = {Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U. and {de Cola}, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and {Perez-Vicente}, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Andrew Jin Shen and Younis, Omar G.},
  year = {2024},
  month = may,
  urldate = {2024-05-01},
  abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
  copyright = {MIT}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  journal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  urldate = {2024-05-01},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  file = {/home/liyouzhou/Zotero/storage/8FRL7MEZ/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@article{wiewioraPotentialBasedShapingQValue2003,
  title = {Potential-{{Based Shaping}} and {{Q-Value Initialization}} Are {{Equivalent}}},
  author = {Wiewiora, E.},
  year = {2003},
  month = sep,
  journal = {jair},
  volume = {19},
  eprint = {1106.5267},
  primaryclass = {cs},
  pages = {205--208},
  issn = {1076-9757},
  doi = {10.1613/jair.1190},
  urldate = {2024-05-03},
  abstract = {Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/liyouzhou/Zotero/storage/833GAZ4C/Wiewiora - 2003 - Potential-Based Shaping and Q-Value Initialization.pdf;/home/liyouzhou/Zotero/storage/EK7WS3KM/1106.html}
}

@misc{zhaDistillingRetrievingGeneralizable2024,
  title = {Distilling and {{Retrieving Generalizable Knowledge}} for {{Robot Manipulation}} via {{Language Corrections}}},
  author = {Zha, Lihan and Cui, Yuchen and Lin, Li-Heng and Kwon, Minae and Arenas, Montserrat Gonzalez and Zeng, Andy and Xia, Fei and Sadigh, Dorsa},
  year = {2024},
  month = mar,
  number = {arXiv:2311.10678},
  eprint = {2311.10678},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.10678},
  urldate = {2024-05-02},
  abstract = {Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results, videos, prompts and code on https://sites.google.com/stanford.edu/droc .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/CGUTREJZ/Zha et al. - 2024 - Distilling and Retrieving Generalizable Knowledge .pdf;/home/liyouzhou/Zotero/storage/WIPSC3TS/2311.html}
}

@article{zhuDeepReinforcementLearning2021,
  title = {Deep Reinforcement Learning Based Mobile Robot Navigation: {{A}} Review},
  shorttitle = {Deep Reinforcement Learning Based Mobile Robot Navigation},
  author = {Zhu, Kai and Zhang, Tao},
  year = {2021},
  month = oct,
  journal = {Tsinghua Science and Technology},
  volume = {26},
  number = {5},
  pages = {674--691},
  issn = {1007-0214},
  doi = {10.26599/TST.2021.9010012},
  urldate = {2024-05-01},
  abstract = {Navigation is a fundamental problem of mobile robots, for which Deep Reinforcement Learning (DRL) has received significant attention because of its strong representation and experience learning abilities. There is a growing trend of applying DRL to mobile robot navigation. In this paper, we review DRL methods and DRL-based navigation frameworks. Then we systematically compare and analyze the relationship and differences between four typical application scenarios: local obstacle avoidance, indoor navigation, multi-robot navigation, and social navigation. Next, we describe the development of DRL-based navigation. Last, we discuss the challenges and some possible solutions regarding DRL-based navigation.},
  keywords = {deep reinforcement learning,mobile robot navigation,Mobile robots,Navigation,obstacle avoidance,Reinforcement learning,Robot sensing systems,Robots,Simultaneous localization and mapping,Task analysis},
  file = {/home/liyouzhou/Zotero/storage/H6RCAHV5/Zhu and Zhang - 2021 - Deep reinforcement learning based mobile robot nav.pdf;/home/liyouzhou/Zotero/storage/DN69VB3R/9409758.html}
}

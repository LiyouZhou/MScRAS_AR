@misc{bohezImitateRepurposeLearning2022,
  title = {Imitate and {{Repurpose}}: {{Learning Reusable Robot Movement Skills From Human}} and {{Animal Behaviors}}},
  shorttitle = {Imitate and {{Repurpose}}},
  author = {Bohez, Steven and Tunyasuvunakool, Saran and Brakel, Philemon and Sadeghi, Fereshteh and Hasenclever, Leonard and Tassa, Yuval and Parisotto, Emilio and Humplik, Jan and Haarnoja, Tuomas and Hafner, Roland and Wulfmeier, Markus and Neunert, Michael and Moran, Ben and Siegel, Noah and Huber, Andrea and Romano, Francesco and Batchelor, Nathan and Casarini, Federico and Merel, Josh and Hadsell, Raia and Heess, Nicolas},
  year = {2022},
  month = mar,
  number = {arXiv:2203.17138},
  eprint = {2203.17138},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.17138},
  urldate = {2024-05-06},
  abstract = {We investigate the use of prior knowledge of human and animal movement to learn reusable locomotion skills for real legged robots. Our approach builds upon previous work on imitating human or dog Motion Capture (MoCap) data to learn a movement skill module. Once learned, this skill module can be reused for complex downstream tasks. Importantly, due to the prior imposed by the MoCap data, our approach does not require extensive reward engineering to produce sensible and natural looking behavior at the time of reuse. This makes it easy to create well-regularized, task-oriented controllers that are suitable for deployment on real robots. We demonstrate how our skill module can be used for imitation, and train controllable walking and ball dribbling policies for both the ANYmal quadruped and OP3 humanoid. These policies are then deployed on hardware via zero-shot simulation-to-reality transfer. Accompanying videos are available at https://bit.ly/robot-npmp.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/U9AMK2B2/Bohez et al. - 2022 - Imitate and Repurpose Learning Reusable Robot Mov.pdf;/home/liyouzhou/Zotero/storage/DBYRCYH2/2203.html}
}

@misc{CarnegieMellonUniversity,
  title = {Carnegie {{Mellon University}} - {{CMU Graphics Lab}} - Motion Capture Library},
  urldate = {2024-05-08},
  howpublished = {http://mocap.cs.cmu.edu/},
  file = {/home/liyouzhou/Zotero/storage/DNXEF622/mocap.cs.cmu.edu.html}
}

@misc{correiaSurveyDemonstrationLearning2023,
  title = {A {{Survey}} of {{Demonstration Learning}}},
  author = {Correia, Andr{\'e} and Alexandre, Lu{\'i}s A.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.11191},
  eprint = {2303.11191},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11191},
  urldate = {2024-05-05},
  abstract = {With the fast improvement of machine learning, reinforcement learning (RL) has been used to automate human tasks in different areas. However, training such agents is difficult and restricted to expert users. Moreover, it is mostly limited to simulation environments due to the high cost and safety concerns of interactions in the real world. Demonstration Learning is a paradigm in which an agent learns to perform a task by imitating the behavior of an expert shown in demonstrations. It is a relatively recent area in machine learning, but it is gaining significant traction due to having tremendous potential for learning complex behaviors from demonstrations. Learning from demonstration accelerates the learning process by improving sample efficiency, while also reducing the effort of the programmer. Due to learning without interacting with the environment, demonstration learning would allow the automation of a wide range of real world applications such as robotics and healthcare. This paper provides a survey of demonstration learning, where we formally introduce the demonstration problem along with its main challenges and provide a comprehensive overview of the process of learning from demonstrations from the creation of the demonstration data set, to learning methods from demonstrations, and optimization by combining demonstration learning with different machine learning methods. We also review the existing benchmarks and identify their strengths and limitations. Additionally, we discuss the advantages and disadvantages of the paradigm as well as its main applications. Lastly, we discuss our perspective on open problems and research directions for this rapidly growing field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/liyouzhou/Zotero/storage/W55DD6TI/Correia and Alexandre - 2023 - A Survey of Demonstration Learning.pdf;/home/liyouzhou/Zotero/storage/TUIFX948/2303.html}
}

@misc{daiAutomaticCurriculaExpert2022,
  title = {Automatic {{Curricula}} via {{Expert Demonstrations}}},
  author = {Dai, Siyu and Hofmann, Andreas and Williams, Brian},
  year = {2022},
  month = feb,
  number = {arXiv:2106.09159},
  eprint = {2106.09159},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {We propose Automatic Curricula via Expert Demonstrations (ACED), a reinforcement learning (RL) approach that combines the ideas of imitation learning and curriculum learning in order to solve challenging robotic manipulation tasks with sparse reward functions. Curriculum learning solves complicated RL tasks by introducing a sequence of auxiliary tasks with increasing difficulty, yet how to automatically design effective and generalizable curricula remains a challenging research problem. ACED extracts curricula from a small amount of expert demonstration trajectories by dividing demonstrations into sections and initializing training episodes to states sampled from different sections of demonstrations. Through moving the reset states from the end to the beginning of demonstrations as the learning agent improves its performance, ACED not only learns challenging manipulation tasks with unseen initializations and goals, but also discovers novel solutions that are distinct from the demonstrations. In addition, ACED can be naturally combined with other imitation learning methods to utilize expert demonstrations in a more efficient manner, and we show that a combination of ACED with behavior cloning allows pick-and-place tasks to be learned with as few as 1 demonstration and block stacking tasks to be learned with 20 demonstrations.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/HCEMPRRQ/Dai et al. - 2022 - Automatic Curricula via Expert Demonstrations.pdf}
}

@misc{dewolfStudywolfPydmps2024,
  title = {Studywolf/Pydmps},
  author = {DeWolf, Travis},
  year = {2024},
  month = apr,
  urldate = {2024-05-06},
  copyright = {GPL-3.0}
}

@article{flashMotorPrimitivesVertebrates2005,
  title = {Motor Primitives in Vertebrates and Invertebrates},
  author = {Flash, Tamar and Hochner, Binyamin},
  year = {2005},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  series = {Motor Sytems / {{Neurobiology}} of Behaviour},
  volume = {15},
  number = {6},
  pages = {660--666},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2005.10.011},
  urldate = {2024-05-06},
  abstract = {In recent years different lines of evidence have led to the idea that motor actions and movements in both vertebrates and invertebrates are composed of elementary building blocks. The entire motor repertoire can be spanned by applying a well-defined set of operations and transformations to these primitives and by combining them in many different ways according to well-defined syntactic rules. Motor and movement primitives and modules might exist at the neural, dynamic and kinematic levels with complicated mapping among the elementary building blocks subserving these different levels of representation. Hence, while considerable progress has been made in recent years in unravelling the nature of these primitives, new experimental, computational and conceptual approaches are needed to further advance our understanding of motor compositionality.}
}

@inproceedings{ijspeertMovementImitationNonlinear2002,
  title = {Movement Imitation with Nonlinear Dynamical Systems in Humanoid Robots},
  booktitle = {Proceedings 2002 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{02CH37292}})},
  author = {Ijspeert, A.J. and Nakanishi, J. and Schaal, S.},
  year = {2002},
  month = may,
  volume = {2},
  pages = {1398-1403 vol.2},
  doi = {10.1109/ROBOT.2002.1014739},
  urldate = {2024-05-08},
  abstract = {Presents an approach to movement planning, on-line trajectory modification, and imitation learning by representing movement plans based on a set of nonlinear differential equations with well-defined attractor dynamics. The resultant movement plan remains an autonomous set of nonlinear differential equations that forms a control policy (CP) which is robust to strong external perturbations and that can be modified on-line by additional perceptual variables. We evaluate the system with a humanoid robot simulation and an actual humanoid robot. Experiments are presented for the imitation of three types of movements: reaching movements with one arm, drawing movements of 2-D patterns, and tennis swings. Our results demonstrate (a) that multi-joint human movements can be encoded successfully by the CPs, (b) that a learned movement policy can readily be reused to produce robust trajectories towards different targets, (c) that a policy fitted for one particular target provides a good predictor of human reaching movements towards neighboring targets, and (d) that the parameter space which encodes a policy is suitable for measuring to which extent two trajectories are qualitatively similar.},
  keywords = {Biological system modeling,Control systems,Convergence,Encoding,Humanoid robots,Humans,Laboratories,Nonlinear dynamical systems,Robustness,Trajectory},
  file = {/home/liyouzhou/Zotero/storage/Y47AQXPS/Ijspeert et al. - 2002 - Movement imitation with nonlinear dynamical system.pdf;/home/liyouzhou/Zotero/storage/ZKU94RR6/figures.html}
}

@inproceedings{ijspeertTrajectoryFormationImitation2001,
  title = {Trajectory Formation for Imitation with Nonlinear Dynamical Systems},
  booktitle = {Proceedings 2001 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}. {{Expanding}} the {{Societal Role}} of {{Robotics}} in the the {{Next Millennium}} ({{Cat}}. {{No}}.{{01CH37180}})},
  author = {Ijspeert, A.J. and Nakanishi, J. and Schaal, S.},
  year = {2001},
  month = oct,
  volume = {2},
  pages = {752-757 vol.2},
  doi = {10.1109/IROS.2001.976259},
  urldate = {2024-05-06},
  abstract = {Explores an approach to learning by imitation and trajectory formation by representing movements as mixtures of nonlinear differential equations with well-defined attractor dynamics. An observed movement is approximated by finding a best fit of the mixture model to its data by a recursive least squares regression technique. In contrast to non-autonomous movement representations like splines, the resultant movement plan remains an autonomous set of nonlinear differential equations that forms a control policy which is robust to strong external perturbations and that can be modified by additional perceptual variables. This movement policy remains the same for a given target, regardless of the initial conditions, and can easily be re-used for new targets. We evaluate the trajectory formation system in the context of a humanoid robot simulation that is part of the Virtual Trainer project, which aims at supervising rehabilitation exercises in stroke-patients. A typical rehabilitation exercise was collected with a Sarcos Sensuit, a device to record joint angular movement from human subjects, and approximated and reproduced with our imitation techniques. Our results demonstrate that multijoint human movements can be encoded successfully, and that this system allows robust modifications of the,movement policy through external variables.},
  keywords = {Control systems,Humanoid robots,Laboratories,Least squares approximation,Motor drives,Neural networks,Nonlinear dynamical systems,Robustness,Spline,Trajectory},
  file = {/home/liyouzhou/Zotero/storage/9BSIK4YA/Ijspeert et al. - 2001 - Trajectory formation for imitation with nonlinear .pdf;/home/liyouzhou/Zotero/storage/JAQ9SRN8/976259.html}
}

@article{khansari-zadehLearningStableNonlinear2011,
  title = {Learning {{Stable Nonlinear Dynamical Systems With Gaussian Mixture Models}}},
  author = {{Khansari-Zadeh}, S. Mohammad and Billard, Aude},
  year = {2011},
  month = oct,
  journal = {IEEE Transactions on Robotics},
  volume = {27},
  number = {5},
  pages = {943--957},
  issn = {1941-0468},
  doi = {10.1109/TRO.2011.2159412},
  urldate = {2024-05-06},
  abstract = {This paper presents a method to learn discrete robot motions from a set of demonstrations. We model a motion as a nonlinear autonomous (i.e., time-invariant) dynamical system (DS) and define sufficient conditions to ensure global asymptotic stability at the target. We propose a learning method, which is called Stable Estimator of Dynamical Systems (SEDS), to learn the parameters of the DS to ensure that all motions closely follow the demonstrations while ultimately reaching and stopping at the target. Time-invariance and global asymptotic stability at the target ensures that the system can respond immediately and appropriately to perturbations that are encountered during the motion. The method is evaluated through a set of robot experiments and on a library of human handwriting motions.},
  keywords = {Asymptotic stability,Dynamical systems (DS),Dynamics,Gaussian mixture model,imitation learning,Numerical stability,point-to-point motions,Robot kinematics,stability analysis,Stability analysis,Trajectory},
  file = {/home/liyouzhou/Zotero/storage/YDLRXRU7/Khansari-Zadeh and Billard - 2011 - Learning Stable Nonlinear Dynamical Systems With G.pdf;/home/liyouzhou/Zotero/storage/YD2D2Q4V/5953529.html}
}

@inproceedings{maedaLearningInteractionCollaborative2014,
  title = {Learning Interaction for Collaborative Tasks with Probabilistic Movement Primitives},
  booktitle = {2014 {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Maeda, Guilherme and Ewerton, Marco and Lioutikov, Rudolf and Ben Amor, Heni and Peters, Jan and Neumann, Gerhard},
  year = {2014},
  month = nov,
  pages = {527--534},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2014.7041413},
  urldate = {2024-05-06},
  abstract = {This paper proposes a probabilistic framework based on movement primitives for robots that work in collaboration with a human coworker. Since the human coworker can execute a variety of unforeseen tasks a requirement of our system is that the robot assistant must be able to adapt and learn new skills on-demand, without the need of an expert programmer. Thus, this paper leverages on the framework of imitation learning and its application to human-robot interaction using the concept of Interaction Primitives (IPs). We introduce the use of Probabilistic Movement Primitives (ProMPs) to devise an interaction method that both recognizes the action of a human and generates the appropriate movement primitive of the robot assistant. We evaluate our method on experiments using a lightweight arm interacting with a human partner and also using motion capture trajectories of two humans assembling a box. The advantages of ProMPs in relation to the original formulation for interaction are exposed and compared.},
  keywords = {Collaboration,Hidden Markov models,Probabilistic logic,Robot kinematics,Trajectory,Vectors},
  file = {/home/liyouzhou/Zotero/storage/NI973EGP/Maeda et al. - 2014 - Learning interaction for collaborative tasks with .pdf;/home/liyouzhou/Zotero/storage/GZC42DGM/7041413.html}
}

@misc{merelHierarchicalVisuomotorControl2019,
  title = {Hierarchical Visuomotor Control of Humanoids},
  author = {Merel, Josh and Ahuja, Arun and Pham, Vu and Tunyasuvunakool, Saran and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Wayne, Greg},
  year = {2019},
  month = jan,
  number = {arXiv:1811.09656},
  eprint = {1811.09656},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.09656},
  urldate = {2024-05-08},
  abstract = {We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/8IXG7ZS7/Merel et al. - 2019 - Hierarchical visuomotor control of humanoids.pdf;/home/liyouzhou/Zotero/storage/6HYG9FE3/1811.html}
}

@misc{merelLearningHumanBehaviors2017,
  title = {Learning Human Behaviors from Motion Capture by Adversarial Imitation},
  author = {Merel, Josh and Tassa, Yuval and TB, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
  year = {2017},
  month = jul,
  number = {arXiv:1707.02201},
  eprint = {1707.02201},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to produce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/liyouzhou/Zotero/storage/HNQSSN9A/Merel et al. - 2017 - Learning human behaviors from motion capture by ad.pdf}
}

@misc{merelNeuralProbabilisticMotor2019,
  title = {Neural Probabilistic Motor Primitives for Humanoid Control},
  author = {Merel, Josh and Hasenclever, Leonard and Galashov, Alexandre and Ahuja, Arun and Pham, Vu and Wayne, Greg and Teh, Yee Whye and Heess, Nicolas},
  year = {2019},
  month = jan,
  number = {arXiv:1811.11711},
  eprint = {1811.11711},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.11711},
  urldate = {2024-05-06},
  abstract = {We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Additionally, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and the resulting movements are relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video ( https://youtu.be/CaDEf-QcKwA ) summarizing our results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/DN2BDUY9/Merel et al. - 2019 - Neural probabilistic motor primitives for humanoid.pdf;/home/liyouzhou/Zotero/storage/QZ8USYBZ/1811.html}
}

@misc{mohtasibRobotPolicyLearning2022,
  title = {Robot {{Policy Learning}} from {{Demonstration Using Advantage Weighting}} and {{Early Termination}}},
  author = {Mohtasib, Abdalkarim and Neumann, Gerhard and Cuayahuitl, Heriberto},
  year = {2022},
  month = jul,
  number = {arXiv:2208.00478},
  eprint = {2208.00478},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {Learning robotic tasks in the real world is still highly challenging and effective practical solutions remain to be found. Traditional methods used in this area are imitation learning and reinforcement learning, but they both have limitations when applied to real robots. Combining reinforcement learning with pre-collected demonstrations is a promising approach that can help in learning control policies to solve robotic tasks. In this paper, we propose an algorithm that uses novel techniques to leverage offline expert data using offline and online training to obtain faster convergence and improved performance. The proposed algorithm (AWET) weights the critic losses with a novel agent advantage weight to improve over the expert data. In addition, AWET makes use of an automatic early termination technique to stop and discard policy rollouts that are not similar to expert trajectories---to prevent drifting far from the expert data. In an ablation study, AWET showed improved and promising performance when compared to state-of-the-art baselines on four standard robotic tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/KZDCASF8/Mohtasib et al. - 2022 - Robot Policy Learning from Demonstration Using Adv.pdf}
}

@article{mussa-ivaldiModularFeaturesMotor1999,
  title = {Modular Features of Motor Control and Learning},
  author = {{Mussa-Ivaldi}, F. A.},
  year = {1999},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  volume = {9},
  number = {6},
  pages = {713--717},
  issn = {0959-4388},
  doi = {10.1016/s0959-4388(99)00029-x},
  abstract = {The study of complex motor behaviours has highlighted the role of modular representations both in the planning and in the execution of actions. Recent findings suggest the presence of functional modules within a variety of neural structures. Computational investigations are now addressing the issue of how these modules may act concurrently to generate a wide repertoire of behaviours.},
  langid = {english},
  pmid = {10607638},
  keywords = {Animals,Extremities,Humans,Learning,Motor Activity,Nervous System Physiological Phenomena}
}

@misc{OneStreamVideo,
  title = {One {{Stream Video}}},
  urldate = {2024-05-05},
  abstract = {Abstract We introduce a framework for online learning from a single continuous video stream -- the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little},
  howpublished = {https://sites.google.com/view/one-stream-video},
  langid = {british},
  file = {/home/liyouzhou/Zotero/storage/J743G7VN/one-stream-video.html}
}

@inproceedings{queisserIncrementalBootstrappingParameterized2016,
  title = {Incremental Bootstrapping of Parameterized Motor Skills},
  booktitle = {2016 {{IEEE-RAS}} 16th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Quei{\ss}er, Jeffrey Frederic and Reinhart, Ren{\'e} Felix and Steil, Jochen Jakob},
  year = {2016},
  month = nov,
  pages = {223--229},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2016.7803281},
  urldate = {2024-05-08},
  abstract = {Many motor skills have an intrinsic, low-dimensional parameterization, e.g. reaching through a grid to different targets. Repeated policy search for new parameterizations of such a skill is inefficient, because the structure of the skill variability is not exploited. This issue has been previously addressed by learning mappings from task parameters to policy parameters. In this work, we introduce a bootstrapping technique that establishes such parameterized skills incrementally. The approach combines iterative learning with state-of-the-art black-box policy optimization. We investigate the benefits of incrementally learning parameterized skills for efficient policy retrieval and show that the number of required rollouts can be significantly reduced when optimizing policies for novel tasks. The approach is demonstrated for several parameterized motor tasks including upper-body reaching motion generation for the humanoid robot COMAN.},
  keywords = {End effectors,Humanoid robots,Learning (artificial intelligence),Optimization,Training,Trajectory},
  file = {/home/liyouzhou/Zotero/storage/V3FVDHYI/Quei√üer et al. - 2016 - Incremental bootstrapping of parameterized motor s.pdf;/home/liyouzhou/Zotero/storage/2RHKE6DZ/7803281.html}
}

@article{ravichandarRecentAdvancesRobot2020,
  title = {Recent {{Advances}} in {{Robot Learning}} from {{Demonstration}}},
  author = {Ravichandar, Harish and Polydoros, Athanasios S. and Chernova, Sonia and Billard, Aude},
  year = {2020},
  month = may,
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {3},
  number = {Volume 3, 2020},
  pages = {297--330},
  publisher = {Annual Reviews},
  issn = {2573-5144},
  doi = {10.1146/annurev-control-100819-063206},
  urldate = {2024-05-05},
  abstract = {In the context of robotics and automation, learning from demonstration (LfD) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of LfD over other robot learning methods is compelling when ideal behavior can be neither easily scripted (as is done in traditional robot programming) nor easily defined as an optimization problem, but can be demonstrated. While there have been multiple surveys of this field in the past, there is a need for a new one given the considerable growth in the number of publications in recent years. This review aims to provide an overview of the collection of machine-learning methods used to enable a robot to learn from and imitate a teacher. We focus on recent advancements in the field and present an updated taxonomy and characterization of existing methods. We also discuss mature and emerging application areas for LfD and highlight the significant challenges that remain to be overcome both in theory and in practice.},
  langid = {english},
  file = {/home/liyouzhou/Zotero/storage/JHSR9DLU/Ravichandar et al. - 2020 - Recent Advances in Robot Learning from Demonstrati.pdf;/home/liyouzhou/Zotero/storage/8KRLHRN8/annurev-control-100819-063206.html}
}

@misc{salimansLearningMontezumaRevenge2018,
  title = {Learning {{Montezuma}}'s {{Revenge}} from a {{Single Demonstration}}},
  author = {Salimans, Tim and Chen, Richard},
  year = {2018},
  month = dec,
  number = {arXiv:1812.03381},
  eprint = {1812.03381},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.03381},
  urldate = {2024-05-06},
  abstract = {We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/liyouzhou/Zotero/storage/HBVAJATM/Salimans and Chen - 2018 - Learning Montezuma's Revenge from a Single Demonst.pdf;/home/liyouzhou/Zotero/storage/AZ4VZ5MC/1812.html}
}

@article{salvadorAccurateDynamicTime2007c,
  title = {Toward Accurate Dynamic Time Warping in Linear Time and Space},
  author = {Salvador, Stan and Chan, Philip},
  year = {2007},
  journal = {Intelligent Data Analysis},
  volume = {11},
  number = {5},
  pages = {561--580},
  publisher = {IOS Press},
  isbn = {1088-467X}
}

@article{saverianoDynamicMovementPrimitives2023,
  title = {Dynamic {{Movement Primitives}} in {{Robotics}}: {{A Tutorial Survey}}},
  shorttitle = {Dynamic {{Movement Primitives}} in {{Robotics}}},
  author = {Saveriano, Matteo and {Abu-Dakka}, Fares J. and Kramberger, Aljaz and Peternel, Luka},
  year = {2023},
  month = nov,
  journal = {The International Journal of Robotics Research},
  volume = {42},
  number = {13},
  eprint = {2102.03861},
  primaryclass = {cs},
  pages = {1133--1184},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783649231201196},
  urldate = {2024-05-06},
  abstract = {Biological systems, including human beings, have the innate ability to perform complex tasks in versatile and agile manner. Researchers in sensorimotor control have tried to understand and formally define this innate property. The idea, supported by several experimental findings, that biological systems are able to combine and adapt basic units of motion into complex tasks finally lead to the formulation of the motor primitives theory. In this respect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical formulation of the motor primitives as stable dynamical systems, and are well suited to generate motor commands for artificial systems like robots. In the last decades, DMPs have inspired researchers in different robotic fields including imitation and reinforcement learning, optimal control,physical interaction, and human-robot co-working, resulting a considerable amount of published papers. The goal of this tutorial survey is two-fold. On one side, we present the existing DMPs formulations in rigorous mathematical terms,and discuss advantages and limitations of each approach as well as practical implementation details. In the tutorial vein, we also search for existing implementations of presented approaches and release several others. On the other side, we provide a systematic and comprehensive review of existing literature and categorize state of the art work on DMP. The paper concludes with a discussion on the limitations of DMPs and an outline of possible research directions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/MD2DDBHQ/Saveriano et al. - 2023 - Dynamic Movement Primitives in Robotics A Tutoria.pdf;/home/liyouzhou/Zotero/storage/K2SF75Y9/2102.html}
}

@article{schaalScalableTechniquesNonparametric2002,
  title = {Scalable {{Techniques}} from {{Nonparametric Statistics}} for {{Real Time Robot Learning}}},
  author = {Schaal, Stefan and Atkeson, Christopher G. and Vijayakumar, Sethu},
  year = {2002},
  month = jul,
  journal = {Applied Intelligence},
  volume = {17},
  number = {1},
  pages = {49--60},
  issn = {1573-7497},
  doi = {10.1023/A:1015727715131},
  urldate = {2024-05-06},
  abstract = {Locally weighted learning (LWL) is a class of techniques from nonparametric statistics that provides useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of robotic systems. This paper introduces several LWL algorithms that have been tested successfully in real-time learning of complex robot tasks. We discuss two major classes of LWL, memory-based LWL and purely incremental LWL that does not need to remember any data explicitly. In contrast to the traditional belief that LWL methods cannot work well in high-dimensional spaces, we provide new algorithms that have been tested on up to 90 dimensional learning problems. The applicability of our LWL algorithms is demonstrated in various robot learning examples, including the learning of devil-sticking, pole-balancing by a humanoid robot arm, and inverse-dynamics learning for a seven and a 30 degree-of-freedom robot. In all these examples, the application of our statistical neural networks techniques allowed either faster or more accurate acquisition of motor control than classical control engineering.},
  langid = {english},
  keywords = {incremental learning,internal models,locally weighted learning,motor control,nonparametric regression},
  file = {/home/liyouzhou/Zotero/storage/BXULDLFT/Schaal et al. - 2002 - Scalable Techniques from Nonparametric Statistics .pdf}
}

@misc{sontakkeRoboCLIPOneDemonstration2023,
  title = {{{RoboCLIP}}: {{One Demonstration}} Is {{Enough}} to {{Learn Robot Policies}}},
  shorttitle = {{{RoboCLIP}}},
  author = {Sontakke, Sumedh A. and Zhang, Jesse and Arnold, S{\'e}bastien M. R. and Pertsch, Karl and B{\i}y{\i}k, Erdem and Sadigh, Dorsa and Finn, Chelsea and Itti, Laurent},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07899},
  eprint = {2310.07899},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-06},
  abstract = {Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration. Visit our website for experiment videos.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/liyouzhou/Zotero/storage/XMCV7QQF/Sontakke et al. - 2023 - RoboCLIP One Demonstration is Enough to Learn Rob.pdf}
}
